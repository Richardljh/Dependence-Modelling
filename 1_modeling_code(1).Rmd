---
title: "Joint modelling for dependent frequency and severity"
author: "Jiahong Li and Guangyuan Gao"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

The ausprivauto0405 dataset is based on one-year vehicle insurance policies taken out in 2004 or 2005. There are 67856 policies, of which 4624 had at least one claim.

```{r data-check}
source("0_process(2).R")
source("0_functions(1).R")
dim(dat1)

emp_fre<-sum(dat1$ClaimNb)/sum(dat1$Exposure) # empirical claims frequency
hist(dat1$Exposure)
dpois(0,emp_fre)+dpois(1,emp_fre)+dpois(2,emp_fre) # very low chance to make more than 2 claims

unique(dat2$ClaimNb)
dim(dat2)
sum(dat1$ClaimNb>0)

dim(dat1_mat)

#png("./plots/box_lnsev.png")
boxplot(LnSev ~ ClaimNb, data=dat2,ylab="logged average claim size", xlab="number of claims")
dev.off()
```

# Marginal modeling 

##  Frequency models

### Poisson regression

```{r}
names(dat1)
ind_test<-5
sum(dat1$ind==ind_test)
names(dat1_mat)
dput(names(dat1_mat2))

#poi_reg <- glm(ClaimNb ~ LnVehValue+VehAgeoldest+
#VehAgeyoung+VehAgeyoungest+VehBodyHatchback+VehBodyWagon+
#GenderMale+DrivAgeold+DrivAgeoldest+DrivAgeworking+
#DrivAgeyoung+DrivAgeyoungest+LnVehValue:VehAgeoldest+
#LnVehValue:VehAgeyoung+LnVehValue:VehAgeyoungest+LnVehValue:VehBodyHatchback+
#LnVehValue:VehBodyWagon+VehAgeyoung:VehBodyHatchback+VehAgeoldest:VehBodyWagon+
#LnVehValue:GenderMale+VehAgeoldest:GenderMale+VehAgeyoung:GenderMale+
#VehBodyHatchback:GenderMale+VehBodyWagon:GenderMale+LnVehValue:DrivAgeold+
#LnVehValue:DrivAgeoldest+LnVehValue:DrivAgeworking+LnVehValue:DrivAgeyoung+
#LnVehValue:DrivAgeyoungest+VehAgeoldest:DrivAgeworking+
#VehBodyWagon:DrivAgeworking+GenderMale:DrivAgeold+GenderMale:DrivAgeworking+
#GenderMale:DrivAgeyoung+LnVehValue:VehAgeyoung:VehBodyHatchback+
#LnVehValue:VehAgeoldest:VehBodyWagon+LnVehValue:VehAgeoldest:GenderMale+
#LnVehValue:VehAgeyoung:GenderMale+LnVehValue:VehBodyHatchback:GenderMale+
#LnVehValue:VehBodyWagon:GenderMale+LnVehValue:VehAgeoldest:DrivAgeworking+
#LnVehValue:VehBodyWagon:DrivAgeworking+LnVehValue:GenderMale:DrivAgeold+
#LnVehValue:GenderMale:DrivAgeworking+LnVehValue:GenderMale:DrivAgeyoung + offset(log(Exposure)) , data = dat1_mat2[dat1_mat2$ind!=ind_test,], family = poisson(link = "log"))

## the selected variables are as follows (stepwise AIC)
# poi_reg <- glm(ClaimNb ~ LnVehValue + VehAgeoldest + VehBodyWagon + 
#     GenderMale + DrivAgeold + DrivAgeoldest + DrivAgeyoungest + 
#     LnVehValue:VehAgeoldest + LnVehValue:GenderMale + LnVehValue:DrivAgeold + 
#     GenderMale:DrivAgeold + LnVehValue:GenderMale:DrivAgeold + offset(log(Exposure)), data = dat1_mat2[dat1_mat2$ind!=ind_test,], family = poisson(link = "log"))

#poi_reg <- glm(ClaimNb ~ LnVehValue + VehAgeoldest + VehAgeyoung + VehAgeyoungest + VehBodyCaravan + VehBodyCoupe + VehBodyHardtop + VehBodyHatchback + VehBodyMinibus + VehBodyOthers + VehBodyTruck + VehBodyUtility + VehBodyVan + VehBodyWagon + GenderMale + DrivAgeold + DrivAgeoldest + DrivAgeworking + DrivAgeyoung + DrivAgeyoungest + offset(log(Exposure)) , data = dat1_mat[dat1_mat$ind!=ind_test,], family = poisson(link = "log"))

# poi_reg <- glm(ClaimNb ~ LnVehValue + VehAge + VehBody + Gender + DrivAge + log(Exposure) + offset(log(Exposure)) , data = dat1[dat1$ind!=ind_test,], family = poisson(link = "log"))

#poi_reg<-step(poi_reg,trace = 1)
#options(digits=5)
#summary(poi_reg,digits=5)

#dat1$PoiNb<-predict(poi_reg,newdata = dat1_mat2,type = "response")
#dat2$PoiNb<-predict(poi_reg, newdata = dat2_mat2,type = "response")
#homo_freq<-sum(dat1$ClaimNb[dat1$ind!=ind_test])/sum(dat1$Exposure[dat1$ind!=ind_test])
#dat1$HomoNb<-dat1$Exposure*homo_freq

#Poisson.Deviance(dat1$PoiNb[dat1$ind==ind_test], dat1$ClaimNb[dat1$ind==ind_test])
#Poisson.Deviance(dat1$HomoNb[dat1$ind==ind_test], dat1$ClaimNb[dat1$ind==ind_test])

poi_reg <- glm(ClaimNb ~ LnVehValue + VehAge + VehBody + Gender + DrivAge +  offset(log(Exposure)) , data = dat1[dat1$ind!=ind_test,], family = poisson(link = "log"))
poi_reg<-step(poi_reg,trace = 0)
options(digits=5)
summary(poi_reg,digits=5)

dat1$PoiNb<-predict(poi_reg,newdata = dat1,type = "response")
dat2$PoiNb<-predict(poi_reg, newdata = dat2,type = "response")
homo_freq<-sum(dat1$ClaimNb[dat1$ind!=ind_test])/sum(dat1$Exposure[dat1$ind!=ind_test])
dat1$HomoNb<-dat1$Exposure*homo_freq

Poisson.Deviance(dat1$PoiNb[dat1$ind==ind_test], dat1$ClaimNb[dat1$ind==ind_test])
Poisson.Deviance(dat1$HomoNb[dat1$ind==ind_test], dat1$ClaimNb[dat1$ind==ind_test])

```

Poisson Regression is better based on Poisson Deviance Loss on test_feq and minus ZTP loglikelihood on test_sev. Due to the predictive performance of Poisson regression, we think three part model proposed by Shi (2015) is not appropriate for these data.

estimation of dispersion parameter
```{r}
summary(glm(ClaimNb ~ LnVehValue + VehAge + VehBody + Gender + DrivAge +  offset(log(Exposure)) , data = dat1[dat1$ind!=ind_test,], family = quasipoisson(link="log")))$dispersion
sum(residuals.glm(poi_reg,type="pearson")^2)/(nrow(dat1[dat1$ind!=ind_test,])-length(poi_reg$coefficients))
```

##  Severity models

### Without claim counts

```{r}
fit_gamma<-fitdist(dat2$ClaimSev,"gamma",method="mme")
plot(fit_gamma)
fit_lnorm<-fitdist(dat2$ClaimSev,"lnorm",method="mme")
plot(fit_lnorm)
gofstat(list(fit_gamma,fit_lnorm))

# ga_reg <- glm(ClaimSev ~ LnVehValue + VehAgeoldest + VehAgeyoung + VehAgeyoungest + VehBodyCaravan + VehBodyCoupe + VehBodyHardtop + VehBodyHatchback + VehBodyMinibus + VehBodyOthers + VehBodyTruck + VehBodyUtility + VehBodyVan + VehBodyWagon + GenderMale + DrivAgeold + DrivAgeoldest + DrivAgeworking + DrivAgeyoung + DrivAgeyoungest + Exposure, data = dat2_mat[dat2_mat$ind!=ind_test,], family = Gamma(link="log"), weights=ClaimNb)
ga_reg <- glm(ClaimSev ~ LnVehValue + VehAge + VehBody + Gender + DrivAge + Exposure, data = dat2[dat2$ind!=ind_test,], family = Gamma(link="log"), weights=ClaimNb)
ga_reg<-step(ga_reg,trace=0)
# ln_reg <- glm(LnSev ~ LnVehValue + VehAgeoldest + VehAgeyoung + VehAgeyoungest + VehBodyCaravan + VehBodyCoupe + VehBodyHardtop + VehBodyHatchback + VehBodyMinibus + VehBodyOthers + VehBodyTruck + VehBodyUtility + VehBodyVan + VehBodyWagon + GenderMale + DrivAgeold + DrivAgeoldest + DrivAgeworking + DrivAgeyoung + DrivAgeyoungest + Exposure, data = dat2_mat[dat2_mat$ind!=ind_test,], family = gaussian(link="identity"))
# ln_reg <- glm(LnSev ~ LnVehValue + VehAge + VehBody + Gender + DrivAge + Exposure, data = dat2[dat2$ind!=ind_test,], family = gaussian(link="identity"))

#dput(names(dat2_mat2))
#ln_reg <- glm(LnSev ~ LnVehValue+VehAgeoldest+VehAgeyoung+VehAgeyoungest+ 
#VehBodyHatchback+VehBodyUtility+VehBodyWagon+GenderMale+ 
#DrivAgeold+DrivAgeoldest+DrivAgeworking+DrivAgeyoung+ 
#DrivAgeyoungest+LnVehValue:VehAgeoldest+LnVehValue:VehAgeyoung+ 
#LnVehValue:VehAgeyoungest+LnVehValue:VehBodyHatchback+LnVehValue:VehBodyWagon+ 
#VehAgeyoung:VehBodyHatchback+VehAgeoldest:VehBodyWagon+ 
#LnVehValue:GenderMale+VehAgeoldest:GenderMale+VehAgeyoung:GenderMale+ 
#VehAgeyoungest:GenderMale+VehBodyHatchback:GenderMale+VehBodyWagon:GenderMale+ 
#LnVehValue:DrivAgeold+LnVehValue:DrivAgeoldest+LnVehValue:DrivAgeworking+ 
#LnVehValue:DrivAgeyoung+LnVehValue:DrivAgeyoungest+VehAgeoldest:DrivAgeworking+ 
#VehBodyWagon:DrivAgeworking+VehBodyWagon:DrivAgeyoung+GenderMale:DrivAgeold+ 
#GenderMale:DrivAgeworking+GenderMale:DrivAgeyoung+LnVehValue:VehAgeyoung:VehBodyHatchback+ 
#LnVehValue:VehAgeoldest:VehBodyWagon+LnVehValue:VehAgeoldest:GenderMale+ 
#LnVehValue:VehAgeyoung:GenderMale+LnVehValue:VehAgeyoungest:GenderMale+ 
#LnVehValue:VehBodyHatchback:GenderMale+LnVehValue:VehBodyWagon:GenderMale+ 
#LnVehValue:VehAgeoldest:DrivAgeworking+LnVehValue:VehBodyWagon:DrivAgeworking+ 
#LnVehValue:VehBodyWagon:DrivAgeyoung+LnVehValue:GenderMale:DrivAgeold+ 
#LnVehValue:GenderMale:DrivAgeworking+LnVehValue:GenderMale:DrivAgeyoung + Exposure, data = dat2_mat2[dat2_mat2$ind!=ind_test,], family = gaussian(link="identity"))

ln_reg <- glm(LnSev ~ LnVehValue + VehAge + VehBody + Gender + DrivAge + Exposure, data = dat2[dat2$ind!=ind_test,], family = gaussian(link="identity"))
ln_reg<-step(ln_reg,trace = 0)
summary(ga_reg)
summary(ln_reg)
# pdf("./plots/gamma_qq.pdf")
qqnorm(residuals(ga_reg),main="Q-Q plot for Gamma severity model")
qqline(residuals(ga_reg),lty=2)
# dev.off()

# pdf("./plots/ln_qq.pdf")
qqnorm(residuals(ln_reg),main="Q-Q plot for log-normal severity model")
qqline(residuals(ln_reg),lty=2)
# dev.off()

par(mfrow=c(2,2))
plot(ga_reg)
plot(ln_reg)

dat2$GaSe <- predict(ga_reg,newdata = dat2, type = 'response')
dat1$GaSe <- predict(ga_reg,newdata = dat1, type = 'response')

#dat2$LnSe <- predict(ln_reg,newdata = dat2_mat2, type = 'response')
#dat1$LnSe <- predict(ln_reg,newdata = dat1_mat2, type = 'response')

dat2$LnSe <- predict(ln_reg,newdata = dat2, type = 'response')
dat1$LnSe <- predict(ln_reg,newdata = dat1, type = 'response')
```

### With claim counts (gamma negative relationship; log-normal positive)

```{r}
#ga_reg_n <- glm(ClaimSev ~ LnVehValue + VehAgeoldest + VehAgeyoung + VehAgeyoungest + VehBodyCaravan + VehBodyCoupe + VehBodyHardtop + VehBodyHatchback + VehBodyMinibus + VehBodyOthers + VehBodyTruck + VehBodyUtility + VehBodyVan + VehBodyWagon + GenderMale + DrivAgeold + DrivAgeoldest + DrivAgeworking + DrivAgeyoung + DrivAgeyoungest + Exposure + ClaimNb, data = dat2_mat[dat2_mat$ind!=ind_test,],  family = Gamma(link="log"), weights = ClaimNb)
ga_reg_n <- glm(ClaimSev ~ LnVehValue + VehAge + VehBody + Gender + DrivAge +  ClaimNb, data = dat2[dat2$ind!=ind_test,], family = Gamma(link="log"), weights=ClaimNb)
ga_reg_n<-step(ga_reg_n,trace=0)
#ln_reg_n <- glm(LnSev ~ LnVehValue + VehAgeoldest + VehAgeyoung + VehAgeyoungest + VehBodyCaravan + VehBodyCoupe + VehBodyHardtop + VehBodyHatchback + VehBodyMinibus + VehBodyOthers + VehBodyTruck + VehBodyUtility + VehBodyVan + VehBodyWagon + GenderMale + DrivAgeold + DrivAgeoldest + DrivAgeworking + DrivAgeyoung + DrivAgeyoungest + Exposure + ClaimNb, data = dat2_mat[dat2_mat$ind!=ind_test,],  family = gaussian(link="identity"))
ln_reg_n <- glm(LnSev ~ LnVehValue + VehAge + VehBody + Gender + DrivAge +  ClaimNb, data = dat2[dat2$ind!=ind_test,], family = gaussian(link="identity"))
#ln_reg_n <- glm(LnSev ~ LnVehValue+VehAgeoldest+VehAgeyoung+VehAgeyoungest+ 
#VehBodyHatchback+VehBodyUtility+VehBodyWagon+GenderMale+ 
#DrivAgeold+DrivAgeoldest+DrivAgeworking+DrivAgeyoung+ 
#DrivAgeyoungest+LnVehValue:VehAgeoldest+LnVehValue:VehAgeyoung+ 
#LnVehValue:VehAgeyoungest+LnVehValue:VehBodyHatchback+LnVehValue:VehBodyWagon+ 
#VehAgeyoung:VehBodyHatchback+VehAgeoldest:VehBodyWagon+ 
#LnVehValue:GenderMale+VehAgeoldest:GenderMale+VehAgeyoung:GenderMale+ 
#VehAgeyoungest:GenderMale+VehBodyHatchback:GenderMale+VehBodyWagon:GenderMale+ 
#LnVehValue:DrivAgeold+LnVehValue:DrivAgeoldest+LnVehValue:DrivAgeworking+ 
#LnVehValue:DrivAgeyoung+LnVehValue:DrivAgeyoungest+VehAgeoldest:DrivAgeworking+ 
#VehBodyWagon:DrivAgeworking+VehBodyWagon:DrivAgeyoung+GenderMale:DrivAgeold+ 
#GenderMale:DrivAgeworking+GenderMale:DrivAgeyoung+LnVehValue:VehAgeyoung:VehBodyHatchback+ 
#LnVehValue:VehAgeoldest:VehBodyWagon+LnVehValue:VehAgeoldest:GenderMale+ 
#LnVehValue:VehAgeyoung:GenderMale+LnVehValue:VehAgeyoungest:GenderMale+ 
#LnVehValue:VehBodyHatchback:GenderMale+LnVehValue:VehBodyWagon:GenderMale+ 
#LnVehValue:VehAgeoldest:DrivAgeworking+LnVehValue:VehBodyWagon:DrivAgeworking+ 
#LnVehValue:VehBodyWagon:DrivAgeyoung+LnVehValue:GenderMale:DrivAgeold+ 
#LnVehValue:GenderMale:DrivAgeworking+LnVehValue:GenderMale:DrivAgeyoung + Exposure + ClaimNb, data = dat2_mat2[dat2_mat2$ind!=ind_test,], family = gaussian(link="identity"))
#ln_reg_n<-step(ln_reg_n,trace=1)

ln_reg_n<-step(ln_reg_n,trace=0)
summary(ga_reg_n)
summary(ln_reg_n)
par(mfrow=c(2,2))
plot(ga_reg_n)
plot(ln_reg_n)

dat2$GaSe_N <- predict(ga_reg_n, newdata = dat2, type = 'response')
dat1$GaSe_N <- predict(ga_reg_n, newdata = dat1, type = 'response') 

#dat2$LnSe_N <- predict(ln_reg_n, newdata = dat2_mat, type = 'response')
#dat1$LnSe_N <- predict(ln_reg_n, newdata = dat1_mat, type = 'response')

dat2$LnSe_N <- predict(ln_reg_n, newdata = dat2, type = 'response')
dat1$LnSe_N <- predict(ln_reg_n, newdata = dat1, type = 'response')
```

### Comparison of severity models

```{r}
Gamma.Deviance(dat2$GaSe[dat2$ind==ind_test],dat2$ClaimSev[dat2$ind==ind_test],dat2$ClaimNb[dat2$ind==ind_test])
Gamma.Deviance(dat2$GaSe_N[dat2$ind==ind_test], dat2$ClaimSev[dat2$ind==ind_test],dat2$ClaimNb[dat2$ind==ind_test])

mean((dat2$LnSe[dat2$ind==ind_test]-dat2$LnSev[dat2$ind==ind_test])^2)
mean((dat2$LnSe_N[dat2$ind==ind_test]-dat2$LnSev[dat2$ind==ind_test])^2)
```

# Joint modelling 

## Estimation of $\rho$ by IFM

```{r}
dat2$z_fre<-z1(dat2$PoiNb)
dat2$z_sev<-z2(summary(ga_reg)$dispersion, dat2$GaSe, dat2$ClaimSev, dat2$ClaimNb)
dat2$z_sev_ln<- z2_ln(summary(ln_reg)$dispersion, ln_mu=dat2$LnSe, ln_sev=dat2$LnSev )
dat2_learn<-dat2[dat2$ind!=ind_test,]
dat1_test<-dat1[dat1$ind==ind_test,]

obj_fun<-function(rho){
  rho_f(ind=ifelse(dat2_learn$ClaimNb==1,1,0), z1=dat2_learn$z_fre,z2=dat2_learn$z_sev,rho)
}
obj_fun_ln<-function(rho){
  rho_f(ind=ifelse(dat2_learn$ClaimNb==1,1,0), z1=dat2_learn$z_fre,z2=dat2_learn$z_sev_ln,rho)
}
(rho_hat = optim(par=0, obj_fun, method='Brent', lower=-1, upper =1)$par)
(rho_hat_ln = optim(par=0, obj_fun_ln, method='Brent', lower=-1, upper =1)$par)
coef(ga_reg_n)[length(coef(ga_reg_n))]
coef(ln_reg_n)[length(coef(ln_reg_n))]
TT = 2000
```

## Conditional expectation of logged severity given N

```{r}
lambda_range<-range(dat1$PoiNb)
lambda_range<-seq(lambda_range[1],lambda_range[2],by=0.01)
uu<-(1-dpois(0,lambda_range)-dpois(1,lambda_range))/(1-dpois(0,lambda_range))
diff_sev1<-function(u,rho,sigma){
  rho*sigma/(1-u)*dnorm(qnorm(u))
}

diff_sev2<-function(u,rho,sigma){
  -rho*sigma/(u)*dnorm(qnorm(u))
}

m1<-diff_sev1(uu,rho_hat_ln,sqrt(summary(ln_reg)$dispersion))+mean(dat2$LnSe)
m2<-diff_sev2(uu,rho_hat_ln,sqrt(summary(ln_reg)$dispersion))+mean(dat2$LnSe)

# pdf("./plots/copula_Y.pdf")
plot(lambda_range,m1,type="l",ylim=c(6.6,7.0),xlab="claims frequency",ylab="conditional expection of logged severity",lty=2)
lines(lambda_range,m2,type="l",lty=1)
abline(h=mean(dat2$LnSev),lty=3)
legend("topright",c("given N*=2+","given N*=1", "empirical mean of logged severity"),lty=c(1,2,3))
# dev.off()

# pdf("./plots/copula_diff.pdf")
plot(lambda_range,m2-m1,type="l",xlab="claims frequency",ylab="difference in conditional expectation",lty=1,lwd=1.2)
# dev.off()

mm1<-rep(mean(dat2$LnSe_N)+1* coef(ln_reg_n)[length(coef(ln_reg_n))],length(lambda_range))
mm2<-rep(mean(dat2$LnSe_N)+2* coef(ln_reg_n)[length(coef(ln_reg_n))],length(lambda_range))


```

## Simulation based predictive distributions

### Independent method

```{r}
xlim0<-c(400,900)
breaks0<-seq(400,900,10)
ylim0<-c(0,200)

names(dat1)
actualtotalloss <- sum(dat1$ClaimAmount[dat1$ind==ind_test])
set.seed(1010)

independent_S = matrix(nrow = nrow(dat1_test), ncol = TT)
independent_S_ln = matrix(nrow = nrow(dat1_test), ncol = TT)

for(i in 1:nrow(dat1_test)){
  independent_S[i,] <- pred_condition_S(lambda = dat1_test$PoiNb[i], mu = dat1_test$GaSe[i], phi = (summary(ga_reg))$dispersion, coef_N = 0, TT = TT, dis="gamma")
    if (i%%1000==0){print(i)}
}
independenttotalloss <- apply(independent_S, 2, sum)

for(i in 1:nrow(dat1_test)){
  independent_S_ln[i,] <- pred_condition_S(lambda = dat1_test$PoiNb[i], mu = dat1_test$LnSe[i], phi = (summary(ln_reg))$dispersion, coef_N = 0, TT = TT, dis="lnorm")
    if (i%%1000==0){print(i)}
}
independenttotalloss_ln <- apply(independent_S_ln, 2, sum)

hist(independenttotalloss,  xaxt="n", xlab = 'independent total loss', main = '')
abline(v=actualtotalloss,lwd=4,col="blue")
ecdfindependent <- ecdf(independenttotalloss)
ecdfindependent(actualtotalloss)
(mean(independenttotalloss)-actualtotalloss)/actualtotalloss
ind_percent<-agg_per(simulated = independent_S, actual = dat1_test$ClaimAmount)
hist(ind_percent)

# pdf("./plots/ind_pred.pdf")
hist(independenttotalloss_ln/1000, xlab = 'predicted total claims amount (in thousands) in the test data', main = 'independent modelling',ylab="frequency",xlim = xlim0,breaks=breaks0,ylim = ylim0)
abline(v=actualtotalloss/1000,lwd=3,col=1)
legend("topright",c("actual total claims amount"),lty=1,lwd=3)
box()
# dev.off()

ecdfindependent_ln <- ecdf(independenttotalloss_ln)
ecdfindependent_ln(actualtotalloss)
(mean(independenttotalloss_ln)-actualtotalloss)/actualtotalloss
ind_percent_ln<-agg_per(simulated = independent_S_ln, actual = dat1_test$ClaimAmount)
hist(ind_percent_ln)

da<-data.frame(actual=dat1$ClaimAmount[dat1$ind==ind_test])
da$ind_gamma<-apply(independent_S,1,mean)
da$ind_ln<-apply(independent_S_ln,1,mean)

gini(loss="actual",score=c("ind_gamma","ind_ln"),data=da)
```

### Conditional approach from Garrido et al. (2016)

```{r}
set.seed(1010)

# summary(ga_reg_n)
coef_N<- coef(ga_reg_n)[length(coef(ga_reg_n))]
dat1_test$GaSe_NN<-dat1_test$GaSe_N/exp(dat1_test$ClaimNb*coef_N)

# summary(ln_reg_n)
coef_N_ln<- coef(ln_reg_n)[length(coef(ln_reg_n))]
dat1_test$LnSe_NN<-dat1_test$LnSe_N- dat1_test$ClaimNb*coef_N_ln

condition_S = matrix(nrow = nrow(dat1_test), ncol = TT)
condition_S_ln = matrix(nrow = nrow(dat1_test), ncol = TT)

for(i in 1:nrow(dat1_test)){
  condition_S[i,] <- pred_condition_S(lambda = dat1_test$PoiNb[i], mu = dat1_test$GaSe_NN[i], phi = (summary(ga_reg_n))$dispersion, coef_N=coef_N, TT = TT, dis="gamma")
  if (i%%1000==0){print(i)}
}
conditiontotalloss <- apply(condition_S, 2, sum)

for(i in 1:nrow(dat1_test)){
  condition_S_ln[i,] <- pred_condition_S(lambda = dat1_test$PoiNb[i], mu = dat1_test$LnSe_NN[i], phi = summary(ln_reg_n)$dispersion, coef_N=coef_N_ln, TT = TT, dis="lnorm")
  if (i%%1000==0){print(i)}
}
conditiontotalloss_ln <- apply(condition_S_ln, 2, sum)

hist(conditiontotalloss, xaxt="n", xlab = 'total loss of conditional model', main = '')
abline(v=actualtotalloss,lwd=4,col="blue")
ecdfcondition <- ecdf(conditiontotalloss)
ecdfcondition(actualtotalloss)
(mean(conditiontotalloss)-actualtotalloss)/actualtotalloss
condi_percent<-agg_per(simulated = condition_S, actual = dat1_test$ClaimAmount)
hist(condi_percent)

hist(conditiontotalloss_ln, xaxt="n", xlab = 'total loss of conditional model', main = '')
abline(v=actualtotalloss,lwd=4,col="blue")
ecdfcondition_ln <- ecdf(conditiontotalloss_ln)
ecdfcondition_ln(actualtotalloss)
(mean(conditiontotalloss_ln)-actualtotalloss)/actualtotalloss
condi_percent_ln<-agg_per(simulated = condition_S_ln, actual = dat1_test$ClaimAmount)
hist(condi_percent_ln)

da$cond_gamma<-apply(condition_S,1,mean)
da$cond_ln<-apply(condition_S_ln,1,mean)

gini(loss="actual",score=c("ind_gamma","cond_gamma","ind_ln","cond_ln"),data=da)
gini(loss="actual",score=c("ind_ln","cond_ln"),data=da)
```

### Our predictive distributions 

```{r}
set.seed(1010)
our_S = matrix(nrow = sum(dat1$ind==ind_test), ncol = TT)
our_S_ln = matrix(nrow = sum(dat1$ind==ind_test), ncol = TT)

for(i in 1:nrow(dat1_test)){
  our_S[i,] <- pred_S(lambda = dat1_test$PoiNb[i], mu = dat1_test$GaSe[i], phi = (summary(ga_reg))$dispersion, rho_hat=rho_hat, TT = TT, dis="gamma")
  if (i%%1000==0){print(i)}
}

for(i in 1:nrow(dat1_test)){
  our_S_ln[i,] <- pred_S(lambda = dat1_test$PoiNb[i], mu = dat1_test$LnSe[i], phi = (summary(ln_reg))$dispersion, rho_hat=rho_hat_ln, TT = TT, dis="lnorm")
  if (i%%1000==0){print(i)}
}
ourtotalloss <- apply(our_S, 2, sum)
ourtotalloss_ln <- apply(our_S_ln, 2, sum)

hist(ourtotalloss, xaxt="n", xlab = 'total loss of our copula model', main = '')
abline(v=actualtotalloss,lwd=4,col="blue")
ecdfour <- ecdf(ourtotalloss)
ecdfour(actualtotalloss)
(mean(ourtotalloss)-actualtotalloss)/actualtotalloss
our_percent<-agg_per(simulated = our_S, actual = dat1_test$ClaimAmount)
hist(our_percent)

# pdf("./plots/our_pred.pdf")
hist(ourtotalloss_ln/1000, xlab = 'predicted total claims amount (in thousands) in the test data', main = 'our proposed copula model',ylab="frequency",xlim = xlim0,breaks=breaks0,ylim=ylim0)
abline(v=actualtotalloss/1000,lwd=3,col=1)
legend("topright",c("actual total claims amount"),lty=1,lwd=3)
box()
# dev.off()

ecdfour_ln <- ecdf(ourtotalloss_ln)
ecdfour_ln(actualtotalloss)
(mean(ourtotalloss_ln)-actualtotalloss)/actualtotalloss
our_percent_ln<-agg_per(simulated = our_S_ln, actual = dat1_test$ClaimAmount)
hist(our_percent_ln)

da$our_gamma<-apply(our_S,1,mean)
da$our_ln<-apply(our_S_ln,1,mean)

gini(loss="actual",score=c("ind_ln","cond_ln","our_ln"),data=da)
```

### Copula approach of Kramer (2013) 

estimation of $\rho$ by IFM

```{r}
(kramer_rho <-ifm_rho_kramer(
  ClaimNb = dat2_learn$ClaimNb,
  lambda = dat2_learn$PoiNb,
  Sev = dat2_learn$ClaimSev,
  mu = dat2_learn$GaSe, # mean of gamma distribution
  phi = (summary(ga_reg))$dispersion, # dispersion parameter of gamma distribution
  ln=F))
(kramer_rho_ln <-ifm_rho_kramer(
  ClaimNb = dat2_learn$ClaimNb,
  lambda = dat2_learn$PoiNb,
  Sev = dat2_learn$LnSev,
  mu = dat2_learn$LnSe, # mean of gamma distribution
  phi = (summary(ln_reg))$dispersion, # dispersion parameter of gamma distribution
  ln=T))
```


```{r}
set.seed(1010)
kramer_S = matrix(nrow = nrow(dat1_test), ncol = TT)
kramer_S_ln = matrix(nrow = nrow(dat1_test), ncol = TT)

for(i in 1:nrow(dat1_test)){
  kramer_S[i,] <- pred_kramer_S(lambda = dat1_test$PoiNb[i], mu = dat1_test$GaSe[i], phi = (summary(ga_reg))$dispersion, rho=kramer_rho, TT = TT, ln=F)
  if (i%%1000==0){print(i)}
}
kramertotalloss <- apply(kramer_S, 2, sum)

for(i in 1:nrow(dat1_test)){
  kramer_S_ln[i,] <- pred_kramer_S(lambda = dat1_test$PoiNb[i], mu = dat1_test$LnSe[i], phi = (summary(ln_reg))$dispersion, rho=kramer_rho_ln, TT = TT,ln=T)
  if (i%%1000==0){print(i)}
}
kramertotalloss_ln <- apply(kramer_S_ln, 2, sum)

hist(kramertotalloss, xaxt="n", xlab = 'total loss of kramer copula model', main = '')
abline(v=actualtotalloss,lwd=4,col="blue")
ecdfkramer <- ecdf(kramertotalloss)
ecdfkramer(actualtotalloss)
(mean(kramertotalloss)-actualtotalloss)/actualtotalloss
kramer_percent<-agg_per(simulated = kramer_S, actual = dat1_test$ClaimAmount)
hist(kramer_percent)

hist(kramertotalloss_ln, xaxt="n", xlab = 'total loss of kramer copula model', main = '')
abline(v=actualtotalloss,lwd=4,col="blue")
ecdfkramer_ln <- ecdf(kramertotalloss_ln)
ecdfkramer_ln(actualtotalloss)
(mean(kramertotalloss_ln)-actualtotalloss)/actualtotalloss
kramer_percent_ln<-agg_per(simulated = kramer_S_ln, actual = dat1_test$ClaimAmount)
hist(kramer_percent_ln)

da$kramer_gamma<-apply(kramer_S,1,mean)
da$kramer_ln<-apply(kramer_S_ln,1,mean)

gini(loss="actual",score=c("ind_ln","cond_ln","kramer_ln","our_ln"),data=da)
gini(loss="actual",score=c("ind_ln","cond_ln","our_ln","kramer_ln","ind_gamma","cond_gamma","our_gamma","kramer_gamma"),data=da)


ks.test(jitter(ind_percent_ln),"punif")
ks.test(jitter(condi_percent_ln),"punif")
ks.test(jitter(kramer_percent_ln),"punif")
ks.test(jitter(our_percent_ln),"punif")
```

```{r}
ifm_rho_t <- function(ClaimSev, ClaimNb, lambda, mu, phi, sev.family, nu){
           u_1 = (1-dpois(0,lambda)-dpois(1,lambda))/(1-dpois(0,lambda))#the $Pr(N^*=2+)$
           z_1 = qt(u_1, df = nu) 
           switch (sev.family ,
                   "gamma"= {u_2 = pgamma(q = ClaimSev, shape = ClaimNb/phi, rate = ClaimNb/(phi*mu))},
                   "lnorm"= {u_2 = plnorm(q = ClaimSev, meanlog=mu, sdlog = phi)},
                   stop("Wrong 'sev.family'")
           )
           z_2 = qt(u_2, df = nu) 
           obj_fun <- function(g_rho){
             rho = (exp(2*g_rho)-1)/(exp(2*g_rho)+1)
             ind_1 = ifelse(ClaimNb==1, 1, 0)
             ind_2 = ifelse(ClaimNb==2, 1, 0)
             logp_1 = pt(q = -(z_1-rho*z_2)/sqrt((1-rho^2)*(nu+z_2^2)/(nu+1)), df = nu+1, log.p = TRUE)
             logp_2 = pt(q = (z_1-rho*z_2)/sqrt((1-rho^2)*(nu+z_2^2)/(nu+1)), df = nu+1, log.p = TRUE)
             obj_value = -sum(ind_1*logp_1+ind_2*logp_2)
             return(obj_value)
           }
           g_rho = (optim(par=0.2, obj_fun, method='BFGS'))$par
           rho = (exp(2*g_rho)-1)/(exp(2*g_rho)+1)
           return(rho)
}
```


```{r}
(rho_hat_t_10 = ifm_rho_t(ClaimSev = dat2$ClaimSev[dat2$ind!=ind_test],
          ClaimNb = dat2$ClaimNb[dat2$ind!=ind_test],
          lambda = dat2$PoiNb[dat2$ind!=ind_test],
          mu = dat2$LnSe[dat2$ind!=ind_test],
          phi = (summary(ln_reg))$dispersion,
          sev.family = 'lnorm',
          nu = 10))

(rho_hat_t_15 = ifm_rho_t(ClaimSev = dat2$ClaimSev[dat2$ind!=ind_test],
          ClaimNb = dat2$ClaimNb[dat2$ind!=ind_test],
          lambda = dat2$PoiNb[dat2$ind!=ind_test],
          mu = dat2$LnSe[dat2$ind!=ind_test],
          phi = (summary(ln_reg))$dispersion,
          sev.family = 'lnorm',
          nu = 15))

(rho_hat_t_20 = ifm_rho_t(ClaimSev = dat2$ClaimSev[dat2$ind!=ind_test],
          ClaimNb = dat2$ClaimNb[dat2$ind!=ind_test],
          lambda = dat2$PoiNb[dat2$ind!=ind_test],
          mu = dat2$LnSe[dat2$ind!=ind_test],
          phi = (summary(ln_reg))$dispersion,
          sev.family = 'lnorm',
          nu = 20))
```


```{r}
pred_S_t <- function(lambda, mu, phi, rho_hat, TT, dis, nu){
  S <- rep(0, TT) ## aggregated claims amount
  for(i in 1:TT){
    T_1 = rexp(1, lambda)
    if(T_1 > 1){
      S[i] = 0
    }
    else{
      z = rStudent(n=1, df = nu, c(0,0), matrix(c(1, rho_hat, rho_hat, 1), 2, 2))# Generate  $(z_{i,1},z_{i,2})$ from the bivariate t distribution with correlation $\hat{\rho}$
      t = inverse_cdf_T(pt(z[1], df = nu), lambda = lambda)# Recover $T_{n+1}$ by inverting its distribution function
      N = 1
      while(t <= 1){
        N = N + 1
        t = t + rexp(1, lambda)
      }
      # if (t<=1) {N=2}
      # if (t>1) {N=1}
      if (dis=="gamma"){
         alpha = 1/phi # shape parameter of gamma distribution
         beta = 1/(phi*mu) # rate parameter of gamma distribution
         S[i] = N*qgamma(pt(z[2], df = nu), shape = alpha*N, rate = beta*N)
        }
      if (dis=="lnorm"){
          S[i] = N*exp(qnorm(pt(z[2], df = nu), mean=mu, sd=sqrt(phi) ) )
      }
      if (dis=="lgamma"){
        alpha = 1/phi # shape parameter of gamma distribution
        beta = 1/(phi*mu) # rate parameter of gamma distribution
        S[i] = N*exp(qgamma(pt(z[2], df = nu), shape=alpha, rate=beta ) )
      }
      }
    }
  return(S)
}
```

```{r}
library(nvmix)
our_S_t_ln <- matrix(nrow = sum(dat1$ind==ind_test), ncol = TT)
for(i in 1:nrow(dat1_test)){
  our_S_t_ln[i,] <- pred_S_t(lambda = dat1_test$PoiNb[i], mu = dat1_test$LnSe[i], phi = (summary(ln_reg))$dispersion, rho_hat=rho_hat_t_10, TT = TT, dis="lnorm", nu =10)
  if (i%%1000==0){print(i)}
}
#ourtotalloss_ln_t <- apply(our_S_t_ln, 2, sum)
#hist(ourtotalloss_ln_t, xaxt="n", xlab = 'total loss of t copula model', main = '')
#abline(v=actualtotalloss,lwd=4,col="blue")
da$our_ln_t_10<-apply(our_S_t_ln,1,mean)

our_S_t_ln <- matrix(nrow = sum(dat1$ind==ind_test), ncol = TT)
for(i in 1:nrow(dat1_test)){
  our_S_t_ln[i,] <- pred_S_t(lambda = dat1_test$PoiNb[i], mu = dat1_test$LnSe[i], phi = (summary(ln_reg))$dispersion, rho_hat=rho_hat_t_15, TT = TT, dis="lnorm", nu =15)
  if (i%%1000==0){print(i)}
}
da$our_ln_t_15<-apply(our_S_t_ln,1,mean)

our_S_t_ln <- matrix(nrow = sum(dat1$ind==ind_test), ncol = TT)
for(i in 1:nrow(dat1_test)){
  our_S_t_ln[i,] <- pred_S_t(lambda = dat1_test$PoiNb[i], mu = dat1_test$LnSe[i], phi = (summary(ln_reg))$dispersion, rho_hat=rho_hat_t_20, TT = TT, dis="lnorm", nu =20)
  if (i%%1000==0){print(i)}
}
da$our_ln_t_20<-apply(our_S_t_ln,1,mean)
```

```{r}
gini(loss="actual",score=c("our_ln_t_10","our_ln_t_15","our_ln_t_20"),data=da)
gini(loss="actual",score=c("our_ln_t_10","our_ln_t_15","our_ln_t_20","our_ln"),data=da)
```


```{r}
ifm_rho_optim_nu <- function(ClaimSev, ClaimNb, lambda, mu, phi, sev.family){
             obj_fun <- function(delta){
              rho = delta[1]
              nu = delta[2] 
              u_1 = (1-dpois(0,lambda)-dpois(1,lambda))/(1-dpois(0,lambda))#the $Pr(N^*=2+)$
              z_1 = qt(u_1, df = nu) 
              switch (sev.family ,
                   "gamma"= {u_2 = pgamma(q = ClaimSev, shape = ClaimNb/phi, 
                                          rate = ClaimNb/(phi*mu))},
                   "lnorm"= {u_2 = plnorm(q = ClaimSev, meanlog=mu, sdlog = phi)},
                   stop("Wrong 'sev.family'")
           )
           z_2 = qt(u_2, df = nu) 
             ind_1 = ifelse(ClaimNb==1, 1, 0)
             ind_2 = ifelse(ClaimNb==2, 1, 0)
             logp_1 = pt(q = -(z_1-rho*z_2)/sqrt((1-rho^2)*(nu+z_2^2)/(nu+1)), df = nu+1, 
                         log.p = TRUE)
             logp_2 = pt(q = (z_1-rho*z_2)/sqrt((1-rho^2)*(nu+z_2^2)/(nu+1)), df = nu+1, 
                         log.p = TRUE)
             obj_value = -sum(ind_1*logp_1+ind_2*logp_2)
             return(obj_value)
           }
           delta_hat = (optim(par=c(0.2,9), obj_fun, method='L-BFGS-B',
                              lower = c(-0.99,1),
                              upper = c(0.99,1000)))$par
           return(delta_hat)
}
```

```{r}
(delta_hat = ifm_rho_optim_nu(ClaimSev = dat2$ClaimSev[dat2$ind!=ind_test],
          ClaimNb = dat2$ClaimNb[dat2$ind!=ind_test],
          lambda = dat2$PoiNb[dat2$ind!=ind_test],
          mu = dat2$LnSe[dat2$ind!=ind_test],
          phi = (summary(ln_reg))$dispersion,
          sev.family = 'lnorm'))
(nu.optimal = delta_hat[2])
(rho_hat_optimal.nu = delta_hat[1])
```

```{r}
rho_hat_t_10
rho_hat_t_15
rho_hat_t_20
rho_hat_ln
rho_hat_optimal.nu
```

```{r}
kendalls_tau <- function(rho) (2/pi)*asin(rho)
kendalls_tau(rho_hat_t_10)
kendalls_tau(rho_hat_t_15)
kendalls_tau(rho_hat_t_20)
kendalls_tau(rho_hat_ln)
kendalls_tau(rho_hat_optimal.nu)
```

```{r}
tail_depend <- function(nu, rho) 2*pt(q = -sqrt((nu+1)*(1-rho)/(1+rho)), df =nu+1)
tail_depend(10, rho = rho_hat_t_10)
tail_depend(15, rho = rho_hat_t_15)
tail_depend(20, rho = rho_hat_t_20)
tail_depend(nu.optimal, rho = rho_hat_optimal.nu)
```

